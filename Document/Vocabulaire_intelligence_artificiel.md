| Mot (Term)            | Définition (Definition)                                                                 |
|-----------------------|------------------------------------------------------------------------------------------|
| Bidirectional and Auto-Regressive Transformers (BART) | Séquence à séquence modèle de langage large (LLM), qui suit une architecture encodage-décodage. Il utilise l'encodage pour la compréhension contextuelle et le décodage pour générer du texte. |
| Bidirectional Encoder Representations from Transformers (BERT) | Modèle de langage large (LLM), qui utilise une architecture transformateur sans décodeur. Il est exceptionnel pour comprendre le contexte d'un mot dans une phrase, ce qui est crucial pour des tâches nuancées comme l'analyse du sentiment. |
| Data analysis learning with language model for generation and exploration (DALL-E) | Modèle AI développé par OpenAI, connu pour générer des images complexes et créatives à partir de descriptions textuelles en utilisant des techniques d'apprentissage profond. |
| Data loader            | Composant applicatif qui permet un batching et un mélanging efficaces des données, essentiels pour l'entraînement des réseaux neuronaux. Il permet le prétraitement sur demande, optimisant ainsi l'utilisation de la mémoire. |
| Diffusion model       | Modèle AI probabiliste utilisé pour la génération d'images. Un modèle de diffusion est formé pour générer des images en apprenant à enlever du bruit ou à reconstruire des exemples de son jeu de données qui ont été altérés au-delà de la reconnaissance. |
| Fine-tuning            | Ajustement d'un modèle prétrainé pour améliorer ses performances pour une tâche spécifique ou un ensemble de données particulier, permettant ainsi au modèle de générer plus précisément et pertinemment du contenu contextualisé. |
| Generative adversarial network (GAN) | Modèle AI génératif qui peut générer des images à partir de vecteurs d'entrée aléatoires ou d'images de départ. Il est composé d'un générateur et d'un discriminateur, qui fonctionnent en mode compétitif. |
| Generative AI          | Modèles d'apprentissage profond capables de générer du texte, des images et d'autres contenus à haute qualité basés sur les données qu'ils ont été formés à utiliser. Ces modèles sont développés et entraînés pour comprendre les schémas et structures dans les données existantes afin de produire de nouvelles données pertinentes. |
| Generative pre-trained transformers (GPT) | Modèle AI génératif basé sur l'architecture des transformateurs. Il a été prétrainé avec d'énormes quantités de données textuelles et peut prédire et générer des séquences de texte en fonction des schémas appris à partir de ses jeux de données d'entraînement. |
| Hugging Face           | Plateforme qui offre une bibliothèque open source avec des modèles préentraînés et des outils pour faciliter le processus de formation et d'ajustement de modèles AI génératifs. |
| Iterator              | Objet que l'on peut boucler. Il contient des éléments qui peuvent être itérés à travers, et inclut généralement les méthodes iter et next. |
| LangChain             | Cadre open source qui aide à la simplification du développement des applications IA en utilisant de grands modèles de langage (LLMs) |
| Large language models (LLMs) | Modèles de base qui utilisent l'IA et l'apprentissage profond avec de vastes ensembles de données pour générer du texte, traduire des langues et créer divers types de contenu. Ils sont appelés grands modèles de langage en raison de la taille de leurs jeux d'entraînement et du nombre de paramètres. |
| Natural language processing (NLP) | Sous-domaine de l'intelligence artificielle qui traite de l'interaction entre les ordinateurs et les humains dans le langage humain. Il implique la création d'algorithmes et de modèles permettant aux ordinateurs de comprendre et de générer du texte contextuellement pertinent dans le langage humain. |
| NLTK                  | Bibliothèque Python utilisée en traitement automatique du langage naturel (NLP) pour des tâches comme la tokenisation et le traitement du texte. |
| Pydantic              | Bibliothèque Python qui facilite la gestion des données. Elle peut être utilisée pour analyser et valider les données. |
| PyTorch               | Cadre dynamique d'apprentissage profond développé par le laboratoire de recherche en intelligence artificielle de Facebook. Il est une bibliothèque Python bien connue pour sa facilité d'utilisation, son flexibilité et ses graphes de calculs dynamiques. |
| Recurrent neural networks (RNNs) | Réseaux de neurones artificiels qui utilisent des données séquentielles ou temporelles. Les RNN sont utilisés pour résoudre les problèmes de données avec un ordre naturel ou des dépendances dans le temps. |
| SentencePiece         | Algorithme de tokenisation basé sur des sous-mots, qui divise le texte en parties gérables et assigne des identifiants uniques. |
| spaCy                 | Bibliothèque open source utilisée en traitement automatique du langage naturel (NLP). Elle fournit des outils pour des tâches comme la tokenisation et les vecteurs de mots. |
| TensorFlow            | Cadre machine learning open-source. Il offre un ensemble d'outils et de bibliothèques pour faciliter le développement et le déploiement de modèles de machine learning. |
| Text-to-Text Transfer Transformer (T5) | Modèle de langage large basé sur les transformateurs, qui utilise une structure text-to-text. Il utilise l'encodage pour la compréhension contextuelle et le décodage pour générer du texte. |
| Tokenization         | Action consistant à diviser du texte en petits morceaux ou jetons. Les jetons aident un modèle AI génératif à mieux comprendre le texte. |
| Tokenizer             | Programme qui décompose du texte en jetons individuels. |
| Transformers          | Modèles d'apprentissage profond capables de traduire du texte et de la parole en quasi temps réel. Ils prennent des données, comme les mots ou les chiffres, et les passent à travers différentes couches, avec l'information qui circule dans une seule direction. |
| Unigram               | Algorithme de tokenisation basé sur des sous-mots qui divise le texte en petits morceaux en démarrant par une grande liste de possibilités qu'il réduit progressivement en fonction de leur fréquence d'apparition dans le texte. |
| variational autoencoders (VAEs) | Modèle AI génératif fonctionnant sur un cadre encodage-décodage. Le réseau encodeur comprime les données d'entrée dans un espace abstrait simplifié qui capte des caractéristiques essentielles, tandis que le réseau décodeur utilise cette information condensée pour recréer les données originales. |
| WaveNet               | Modèle AI génératif conçu pour la génération de contenu audio. Il peut être utilisé pour des tâches comme la synthèse vocale. |
```